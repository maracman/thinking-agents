import logging
import re
import time
import json
import random
import networkx as nx
import pandas as pd
from .schemas import json_schemas
from .llm_service import llm_service

# Set up logging
logger = logging.getLogger('agent_logger')

def format_prompt(history, agent_description, goal, name, user_name):
    """Format the prompt for the agent based on history and agent properties"""
    
    system_prompt = f"""You are {name}, an intelligent agent with the following description:
{agent_description}

Your goal is: {goal}

Always respond in character. Maintain consistent behavior and personality throughout the conversation.
"""
    
    conversation = ""
    for speaker, message in history:
        if speaker == "user":
            conversation += f"{user_name}: {message}\n"
        else:
            agent_name = speaker
            conversation += f"{agent_name}: {message}\n"
    
    full_prompt = f"{system_prompt}\n\nConversation:\n{conversation}"
    return full_prompt

def format_chat_messages(history, agent_description, goal, name, user_name):
    """Format the history as a list of chat messages for providers that support it"""
    messages = []
    
    # Add system message with agent information
    system_content = f"""You are {name}, an intelligent agent with the following description:
{agent_description}

Your goal is: {goal}

Always respond in character. Maintain consistent behavior and personality throughout the conversation."""
    
    messages.append({"role": "system", "content": system_content})
    
    # Add conversation history
    for speaker, message in history:
        role = "user" if speaker == "user" else "assistant"
        messages.append({"role": role, "content": message})
    
    return messages

def get_agent_response(prompt, agent_name, generation_vars):
    """Generate a response from the agent using the configured LLM"""
    
    try:
        # Check if we're in offline mode
        if generation_vars.get('offline', False):
            # Simulate a response in offline mode
            return f"This is a simulated response from {agent_name}. In production, this would be generated by the AI model with parameters: {json.dumps(generation_vars)}."
        
        # Extract LLM provider configuration
        provider = generation_vars.get('provider', 'local')
        model = generation_vars.get('model')
        temperature = generation_vars.get('temperature', 0.7)
        max_tokens = generation_vars.get('max_tokens', 150)
        top_p = generation_vars.get('top_p', 0.9)
        fallback_to_local = generation_vars.get('fallback_to_local', True)
        
        # Set the provider in the LLM service
        llm_service.set_provider(provider)
        
        # Prepare the options for the LLM request
        options = {
            'model': model,
            'temperature': temperature,
            'max_tokens': max_tokens,
            'top_p': top_p,
            'fallback_to_local': fallback_to_local,
            'offline_allowed': True
        }
        
        # For providers that support chat format, parse the prompt into messages
        if provider in ['openai'] and isinstance(prompt, str):
            # Extract the system prompt and conversation history
            sections = prompt.split("\n\nConversation:\n")
            if len(sections) == 2:
                system_prompt = sections[0]
                conversation = sections[1]
                
                # Create a properly formatted messages list
                messages = [{"role": "system", "content": system_prompt}]
                
                # Parse conversation into messages
                for line in conversation.split('\n'):
                    if not line.strip():
                        continue
                    
                    # Split at the first colon
                    parts = line.split(':', 1)
                    if len(parts) == 2:
                        speaker, content = parts[0].strip(), parts[1].strip()
                        role = "user" if speaker == user_name else "assistant"
                        messages.append({"role": role, "content": content})
                
                # Use the messages format instead of the string prompt
                prompt = messages
        
        # Generate the response
        logger.info(f"Generating response using {provider} provider")
        response = llm_service.complete(prompt, options)
        
        # Clean up response if needed
        response = response.strip()
        
        return response
    
    except Exception as e:
        logger.error(f"Error generating response: {str(e)}")
        # Fallback to offline response
        return f"I apologize, but I encountered an error while processing your request. Error: {str(e)}"

def update_graph(graph, current_node, next_node, success, weight=1.0):
    """Update the agent's decision graph with the result of the last action"""
    
    # Ensure both nodes exist
    if current_node not in graph:
        graph.add_node(current_node)
    if next_node not in graph:
        graph.add_node(next_node)
    
    # Check if edge already exists
    if graph.has_edge(current_node, next_node):
        # Update existing edge
        edge_label = "Go" if success else "NoGo"
        graph[current_node][next_node]['label'] = edge_label
        graph[current_node][next_node]['weight'] = weight
    else:
        # Create new edge
        edge_label = "Go" if success else "NoGo"
        graph.add_edge(current_node, next_node, label=edge_label, weight=weight)
    
    return graph

def extract_decision_info(response, current_location):
    """Extract decision information from an agent's response"""
    
    # In a real implementation, this might use regex or more sophisticated parsing
    # to extract information about decisions made by the agent
    
    # For the simple implementation, create a random next node
    next_location = f"node_{random.randint(1, 100)}"
    success = random.choice([True, False])
    weight = random.uniform(0.1, 1.0)
    
    return next_location, success, weight

def offline_main(history, agents_df, settings, user_name, is_user, agent_mutes, len_last_history):
    """Main function for offline mode (without real LLM)"""
    # Set offline flag in settings
    offline_settings = settings.copy()
    offline_settings['offline'] = True
    
    return main(history, agents_df, offline_settings, user_name, is_user, agent_mutes, len_last_history)

def main(history, agents_df, settings, user_name, is_user, agent_mutes, len_last_history, offline=False):
    """Main function for agent processing"""
    
    logger.info(f"Agent processing started with {len(agents_df)} agents, is_user={is_user}")
    
    # If we're in a user turn, don't generate agent responses
    if is_user:
        logger.info("User turn, not generating agent responses")
        return history, agents_df, []
    
    # If history hasn't changed, don't generate new responses
    if len(history) == len_last_history:
        logger.info("History unchanged, not generating new responses")
        return history, agents_df, []
    
    logs = []
    
    # Choose one agent to respond (in a more complex implementation, 
    # you might have multiple agents respond or choose based on relevance)
    unmuted_agents = [i for i, muted in enumerate(agent_mutes) if not muted]
    
    if not unmuted_agents:
        logger.warning("All agents are muted, no responses generated")
        return history, agents_df, ["All agents are muted"]
    
    agent_idx = random.choice(unmuted_agents)
    agent = agents_df.iloc[agent_idx]
    
    # Use agent-specific generation variables if configured
    generation_vars = agent['generation_variables'].copy() if agent['is_agent_generation_variables'] else settings.copy()
    
    # If offline is True, add it to generation vars
    if offline:
        generation_vars['offline'] = True
    
    # Format the prompt for this agent
    prompt = format_prompt(
        history, 
        agent['description'], 
        agent['goal'], 
        agent['agent_name'], 
        user_name
    )
    
    # Get a response from the agent
    logger.info(f"Generating response for {agent['agent_name']}")
    response = get_agent_response(prompt, agent['agent_name'], generation_vars)
    logs.append(f"Generated response for {agent['agent_name']}")
    
    # Update agent's personal history
    agent_df = pd.DataFrame(agents_df)
    agent_df.at[agent_idx, 'personal_history'] = history + [(agent['agent_name'], response)]
    agent_df.at[agent_idx, 'last_response'] = response
    
    # Update agent's graph
    current_location = agent['current_node_location']
    next_location, success, weight = extract_decision_info(response, current_location)
    
    # Read the existing graph
    graph_path = agent['graph_file_path']
    try:
        G = nx.read_graphml(graph_path)
    except Exception as e:
        logger.error(f"Error reading graph, creating new one: {str(e)}")
        G = nx.DiGraph()
        G.add_node('start')
    
    # Update the graph
    G = update_graph(G, current_location, next_location, success, weight)
    
    # Save the updated graph
    nx.write_graphml(G, graph_path)
    
    # Update agent's current location
    agent_df.at[agent_idx, 'current_node_location'] = next_location
    
    # Add the response to the conversation history
    new_history = history + [(agent['agent_name'], response)]
    
    logger.info(f"Agent processing completed. Graph updated: {current_location} -> {next_location}")
    
    return new_history, agent_df, logs